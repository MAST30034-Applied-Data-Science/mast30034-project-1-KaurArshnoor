{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 13:14:58 WARN Utils: Your hostname, mast30034 resolves to a loopback address: 127.0.1.1; using 45.113.234.45 instead (on interface eth0)\n",
      "22/08/21 13:14:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 13:14:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 Tutorial 3')\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get taxi_weather data for all months\n",
    "taxi_weather_data = spark.read.parquet('../data/curated/taxi_weather_data/final-2018-10.parquet')\n",
    "\n",
    "files = ['final-2018-11.parquet', 'final-2018-12.parquet', 'final-2019-01.parquet', \n",
    "         'final-2019-02.parquet', 'final-2019-03.parquet']\n",
    "\n",
    "for file in files:\n",
    "    taxi_weather_month = spark.read.parquet(f'../data/curated/taxi_weather_data/{file}')\n",
    "    # add each month taxi_weather_data to october dataframe\n",
    "    taxi_weather_data = taxi_weather_data.union(taxi_weather_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_day: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- average_temp: double (nullable = true)\n",
      " |-- average_dew_point: double (nullable = true)\n",
      " |-- average_wind_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the data \n",
    "taxi_weather_data = taxi_weather_data.sample(withReplacement=None, fraction=0.05, seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model\n",
    "#### Using features:\n",
    "- 'passenger_count\n",
    "- 'trip_distance'\n",
    "- 'PULocationID'\n",
    "- 'RatecodeID'\n",
    "- 'average_temp' \n",
    "- 'average_dew_point'\n",
    "- 'average_wind_speed'\n",
    "- 'total_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|PULocationID|PULocation_dummy|\n",
      "+------------+----------------+\n",
      "|         263|     (263,[],[])|\n",
      "|           1| (263,[1],[1.0])|\n",
      "|           4| (263,[4],[1.0])|\n",
      "|           7| (263,[7],[1.0])|\n",
      "|          10|(263,[10],[1.0])|\n",
      "|          11|(263,[11],[1.0])|\n",
      "|          12|(263,[12],[1.0])|\n",
      "|          13|(263,[13],[1.0])|\n",
      "|          14|(263,[14],[1.0])|\n",
      "|          15|(263,[15],[1.0])|\n",
      "|          17|(263,[17],[1.0])|\n",
      "|          18|(263,[18],[1.0])|\n",
      "|          21|(263,[21],[1.0])|\n",
      "|          24|(263,[24],[1.0])|\n",
      "|          25|(263,[25],[1.0])|\n",
      "|          26|(263,[26],[1.0])|\n",
      "|          28|(263,[28],[1.0])|\n",
      "|          32|(263,[32],[1.0])|\n",
      "|          33|(263,[33],[1.0])|\n",
      "|          35|(263,[35],[1.0])|\n",
      "+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encode categorical data - PUlocation, DOlocation before linear regression\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# one hot encoder PULocationID\n",
    "onehot_pu = OneHotEncoder(inputCols=['PULocationID'],outputCols=['PULocation_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the taxi_weather data\n",
    "taxi_weather_data2 = onehot_pu.fit(taxi_weather_data).transform(taxi_weather_data)\n",
    "\n",
    "# display results\n",
    "taxi_weather_data2.select('PULocationID', 'PULocation_dummy').distinct().sort('PULocation_dummy').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+----------+----------------+\n",
      "|PULocationID|PULocation_dummy|RatecodeID|RatecodeID_dummy|\n",
      "+------------+----------------+----------+----------------+\n",
      "|         263|     (263,[],[])|       2.0|   (5,[2],[1.0])|\n",
      "|         263|     (263,[],[])|       5.0|       (5,[],[])|\n",
      "|         263|     (263,[],[])|       1.0|   (5,[1],[1.0])|\n",
      "|           1| (263,[1],[1.0])|       5.0|       (5,[],[])|\n",
      "|           4| (263,[4],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|           7| (263,[7],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          10|(263,[10],[1.0])|       2.0|   (5,[2],[1.0])|\n",
      "|          10|(263,[10],[1.0])|       5.0|       (5,[],[])|\n",
      "|          11|(263,[11],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          12|(263,[12],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          13|(263,[13],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          14|(263,[14],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          14|(263,[14],[1.0])|       5.0|       (5,[],[])|\n",
      "|          15|(263,[15],[1.0])|       5.0|       (5,[],[])|\n",
      "|          15|(263,[15],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          17|(263,[17],[1.0])|       5.0|       (5,[],[])|\n",
      "|          17|(263,[17],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          18|(263,[18],[1.0])|       5.0|       (5,[],[])|\n",
      "|          21|(263,[21],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "|          24|(263,[24],[1.0])|       1.0|   (5,[1],[1.0])|\n",
      "+------------+----------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encoder DOLocationID\n",
    "onehot_rate = OneHotEncoder(inputCols=['RatecodeID'], outputCols=['RatecodeID_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the taxi_weather data\n",
    "taxi_weather_onehot = onehot_rate.fit(taxi_weather_data2).transform(taxi_weather_data2)\n",
    "\n",
    "# display results\n",
    "taxi_weather_onehot.select('PULocationID', 'PULocation_dummy','RatecodeID', 'RatecodeID_dummy').distinct().sort('PULocation_dummy').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from MAST30034: Tutorial 3\n",
    "# VectorAssembler creates new vectors from existing columns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = 'features'\n",
    "input_cols = ['trip_distance', 'PULocationID', 'RatecodeID', 'average_temp', \n",
    "              'average_dew_point', 'average_wind_speed', 'total_amount']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    # which column to combine\n",
    "    inputCols=input_cols, \n",
    "    # How should the combined columns be named\n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "taxi_weather_final = assembler.transform(taxi_weather_onehot.dropna('any'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "train_df, test_df = taxi_weather_final.randomSplit([0.7, 0.3], seed = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 18:40:10 WARN Instrumentation: [a7b833ae] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------------------+\n",
      "|        prediction|tip_amount|            features|\n",
      "+------------------+----------+--------------------+\n",
      "|2.2367436806679435|       3.0|[3.61,43.0,1.0,20...|\n",
      "| 8.332002207304512|       5.5|[17.4,132.0,2.0,2...|\n",
      "|  7.94951035679618|       8.3|[10.8,138.0,1.0,2...|\n",
      "|2.9479861869889814|       0.0|[6.2,43.0,1.0,201...|\n",
      "|0.9300884236261381|       0.5|[0.8,232.0,1.0,20...|\n",
      "|1.5726231907874506|       1.2|[2.2,137.0,1.0,20...|\n",
      "|11.336926527043447|     12.61|[16.99,132.0,2.0,...|\n",
      "|3.6417363641327456|      3.86|[4.3,246.0,1.0,20...|\n",
      "|2.1367908686628847|      2.35|[2.2,230.0,1.0,20...|\n",
      "|1.0364049556699126|      1.55|[0.9,246.0,1.0,20...|\n",
      "|1.8401940582770522|       1.5|[2.7,114.0,1.0,21...|\n",
      "|1.2328581443051017|      1.46|[0.93,186.0,1.0,2...|\n",
      "|0.8580143306347131|      1.25|[1.1,137.0,1.0,21...|\n",
      "|0.8043494297349016|      1.06|[0.43,162.0,1.0,2...|\n",
      "| 2.282663740405829|      2.55|[2.5,90.0,1.0,213...|\n",
      "|1.5624212768414958|      1.65|[0.8,162.0,1.0,21...|\n",
      "|1.9749837201865024|      2.32|[0.87,230.0,1.0,2...|\n",
      "| 2.509219147653506|      3.54|[1.89,142.0,1.0,2...|\n",
      "|1.7142669464748534|      1.95|[1.7,211.0,1.0,21...|\n",
      "|1.0740294837582673|      1.36|[0.86,90.0,1.0,21...|\n",
      "+------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE for linear regression model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1388184043549339"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# train linear regression model on training taxi_weather data\n",
    "lm = LinearRegression(featuresCol='features', labelCol='tip_amount').fit(train_df)\n",
    "\n",
    "# Create predictions for the test data\n",
    "predictions = lm.transform(test_df)\n",
    "\n",
    "# Analyse predictions\n",
    "predictions.select(\"prediction\",\"tip_amount\", \"features\").show(20)\n",
    "# Calculate the RMSE\n",
    "print(\"RMSE for linear regression model: \")\n",
    "RegressionEvaluator(labelCol='tip_amount', metricName='rmse').evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.114543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>-0.344418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RatecodeID</th>\n",
       "      <td>-0.844727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_temp</th>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_dew_point</th>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_wind_speed</th>\n",
       "      <td>0.000289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount</th>\n",
       "      <td>0.246659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    coefficient\n",
       "intercept              0.114543\n",
       "trip_distance         -0.344418\n",
       "PULocationID           0.000364\n",
       "RatecodeID            -0.844727\n",
       "average_temp           0.000061\n",
       "average_dew_point      0.000134\n",
       "average_wind_speed     0.000289\n",
       "total_amount           0.246659"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access coefficients\n",
    "pd.DataFrame(\n",
    "    data=[lm.intercept] + list(lm.coefficients),\n",
    "    index=['intercept'] + input_cols,\n",
    "    columns=['coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for decision tree model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2653997265332835"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "# train decision tree model on training taxi_weather data\n",
    "dt_model = DecisionTreeRegressor(featuresCol ='features', labelCol = 'tip_amount').fit(train_df)\n",
    "\n",
    "# Create predictions for the test data\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Calculate the RMSE\n",
    "print('RMSE for decision tree model: ')\n",
    "RegressionEvaluator(labelCol='tip_amount', metricName=\"rmse\").evaluate(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
