{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 13:14:58 WARN Utils: Your hostname, mast30034 resolves to a loopback address: 127.0.1.1; using 45.113.234.45 instead (on interface eth0)\n",
      "22/08/21 13:14:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 13:14:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/08/21 13:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('MAST30034 Tutorial 3')\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \n",
    "    .config('spark.sql.parquet.cacheMetadata', 'true')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get taxi_weather data for all months\n",
    "taxi_weather_data = spark.read.parquet('../data/curated/taxi_weather_data/final-2018-10.parquet')\n",
    "\n",
    "files = ['final-2018-11.parquet', 'final-2018-12.parquet', 'final-2019-01.parquet', \n",
    "         'final-2019-02.parquet', 'final-2019-03.parquet']\n",
    "\n",
    "for file in files:\n",
    "    taxi_weather_month = spark.read.parquet(f'../data/curated/taxi_weather_data/{file}')\n",
    "    # add each month taxi_weather_data to october dataframe\n",
    "    taxi_weather_data = taxi_weather_data.union(taxi_weather_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickup_day: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- average_temp: double (nullable = true)\n",
      " |-- average_dew_point: double (nullable = true)\n",
      " |-- average_wind_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the data \n",
    "taxi_weather_data = taxi_weather_data.sample(withReplacement=None, fraction=0.05, seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model\n",
    "#### Using features:\n",
    "- 'passenger_count\n",
    "- 'trip_distance'\n",
    "- 'PULocationID'\n",
    "- 'DOLocationID'\n",
    "- 'average_temp' \n",
    "- 'average_dew_point'\n",
    "- 'average_wind_speed'\n",
    "- 'total_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|PULocationID|PULocation_dummy|\n",
      "+------------+----------------+\n",
      "|         263|     (263,[],[])|\n",
      "|           1| (263,[1],[1.0])|\n",
      "|           4| (263,[4],[1.0])|\n",
      "|           7| (263,[7],[1.0])|\n",
      "|          10|(263,[10],[1.0])|\n",
      "|          11|(263,[11],[1.0])|\n",
      "|          12|(263,[12],[1.0])|\n",
      "|          13|(263,[13],[1.0])|\n",
      "|          14|(263,[14],[1.0])|\n",
      "|          15|(263,[15],[1.0])|\n",
      "|          17|(263,[17],[1.0])|\n",
      "|          18|(263,[18],[1.0])|\n",
      "|          21|(263,[21],[1.0])|\n",
      "|          24|(263,[24],[1.0])|\n",
      "|          25|(263,[25],[1.0])|\n",
      "|          26|(263,[26],[1.0])|\n",
      "|          28|(263,[28],[1.0])|\n",
      "|          32|(263,[32],[1.0])|\n",
      "|          33|(263,[33],[1.0])|\n",
      "|          35|(263,[35],[1.0])|\n",
      "+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encode categorical data - PUlocation, DOlocation before linear regression\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# one hot encoder PULocationID\n",
    "onehot_pu = OneHotEncoder(inputCols=['PULocationID'],outputCols=['PULocation_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the taxi_weather data\n",
    "taxi_weather_data2 = onehot_pu.fit(taxi_weather_data).transform(taxi_weather_data)\n",
    "\n",
    "# display results\n",
    "taxi_weather_data2.select('PULocationID', 'PULocation_dummy').distinct().sort('PULocation_dummy').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+------------+----------------+\n",
      "|PULocationID| PULocation_dummy|DOLocationID|DOLocation_dummy|\n",
      "+------------+-----------------+------------+----------------+\n",
      "|         138|(263,[138],[1.0])|         263|     (263,[],[])|\n",
      "|         211|(263,[211],[1.0])|         263|     (263,[],[])|\n",
      "|         148|(263,[148],[1.0])|         263|     (263,[],[])|\n",
      "|         162|(263,[162],[1.0])|         263|     (263,[],[])|\n",
      "|          43| (263,[43],[1.0])|         263|     (263,[],[])|\n",
      "|         140|(263,[140],[1.0])|         263|     (263,[],[])|\n",
      "|         238|(263,[238],[1.0])|         263|     (263,[],[])|\n",
      "|         142|(263,[142],[1.0])|         263|     (263,[],[])|\n",
      "|         263|      (263,[],[])|         263|     (263,[],[])|\n",
      "|         186|(263,[186],[1.0])|         263|     (263,[],[])|\n",
      "|         163|(263,[163],[1.0])|         263|     (263,[],[])|\n",
      "|         107|(263,[107],[1.0])|         263|     (263,[],[])|\n",
      "|         229|(263,[229],[1.0])|         263|     (263,[],[])|\n",
      "|          48| (263,[48],[1.0])|         263|     (263,[],[])|\n",
      "|         141|(263,[141],[1.0])|         263|     (263,[],[])|\n",
      "|         233|(263,[233],[1.0])|         263|     (263,[],[])|\n",
      "|         234|(263,[234],[1.0])|         263|     (263,[],[])|\n",
      "|         100|(263,[100],[1.0])|         263|     (263,[],[])|\n",
      "|         237|(263,[237],[1.0])|         263|     (263,[],[])|\n",
      "|         170|(263,[170],[1.0])|         263|     (263,[],[])|\n",
      "+------------+-----------------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one hot encoder DOLocationID\n",
    "onehot_do = OneHotEncoder(inputCols=['DOLocationID'], outputCols=['DOLocation_dummy'])\n",
    "\n",
    "# apply the one hot encoder to the taxi_weather data\n",
    "taxi_weather_onehot = onehot_do.fit(taxi_weather_data2).transform(taxi_weather_data2)\n",
    "\n",
    "# display results\n",
    "taxi_weather_onehot.select('PULocationID', 'PULocation_dummy','DOLocationID', 'DOLocation_dummy').distinct().sort('DOLocation_dummy').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from MAST30034: Tutorial 3\n",
    "# VectorAssembler creates new vectors from existing columns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = 'features'\n",
    "input_cols = ['passenger_count','trip_distance', 'PULocationID', 'DOLocationID', 'average_temp', \n",
    "              'average_dew_point', 'average_wind_speed', 'total_amount']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    # which column to combine\n",
    "    inputCols=input_cols, \n",
    "    # How should the combined columns be named\n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "taxi_weather_final = assembler.transform(taxi_weather_onehot.dropna('any'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing data\n",
    "train_df, test_df = taxi_weather_final.randomSplit([0.7, 0.3], seed = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 13:56:45 WARN Instrumentation: [5ff26654] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for linear regression model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1505841001981674"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# train linear regression model on training taxi_weather data\n",
    "lm = LinearRegression(featuresCol='features', labelCol='tip_amount').fit(train_df)\n",
    "\n",
    "# Create predictions for the test data\n",
    "predictions = lm.transform(test_df)\n",
    "\n",
    "# Calculate the RMSE\n",
    "print(\"RMSE for linear regression model: \")\n",
    "RegressionEvaluator(labelCol='tip_amount', metricName='rmse').evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-0.771178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>0.015276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>-0.375543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOLocationID</th>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_temp</th>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_dew_point</th>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_wind_speed</th>\n",
       "      <td>0.000389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount</th>\n",
       "      <td>0.246754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    coefficient\n",
       "intercept             -0.771178\n",
       "passenger_count        0.015276\n",
       "trip_distance         -0.375543\n",
       "PULocationID           0.000337\n",
       "DOLocationID           0.000422\n",
       "average_temp           0.000052\n",
       "average_dew_point      0.000221\n",
       "average_wind_speed     0.000389\n",
       "total_amount           0.246754"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access coefficients\n",
    "pd.DataFrame(\n",
    "    data=[lm.intercept] + list(lm.coefficients),\n",
    "    index=['intercept'] + input_cols,\n",
    "    columns=['coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for decision tree model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3165663928044855"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "# train decision tree model on training taxi_weather data\n",
    "dt_model = DecisionTreeRegressor(featuresCol ='features', labelCol = 'tip_amount').fit(train_df)\n",
    "\n",
    "# Create predictions for the test data\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Calculate the RMSE\n",
    "print('RMSE for decision tree model: ')\n",
    "RegressionEvaluator(labelCol='tip_amount', metricName=\"rmse\").evaluate(dt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
